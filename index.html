<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zongyu Lin</title>

  <meta name="author" content="Zongyu Lin">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</heads>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:65%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zongyu (Johnson) Lin„ÄåÊûóÂÆóË£ï„Äç</name>
              </p>
              <p>I am a CS Ph.D at UCLA, co-advised by Prof. Yizhou Sun and Prof. Kaiwei Chang. Before coming to UCLA, <strong>I have spent a year in Moonshot.AI (one of the earliest core members)</strong>, working as a full-time research scientist working on LLM and VideoGen. I was the <strong>major contributor of pre-training large language models with extremely long context</strong>, leading to <a href="https://kimi.moonshot.cn/">KIMI CHAT</a>, achieving the state-of-the-art performance on many long context tasks compared with GPT4 and Claude2 at that time. I completed my bachelor's degree of Electronic Enginnering at Tsinghua University. I worked with <a href="https//kimiyoung.github.io/">Prof. Zhilin Yang</a> from 2021 to 2023.
                <!-- <a href="http://www.cs.umd.edu/~hjs/">Prof. Hanan Samet</a> and <a href="https://infolab.usc.edu/Shahabi/index.html">Prof. Cyrus Shahabi</a>. </p>  -->
              <p>My research interest lies broadly in scalable deep generative architecture and LLM reasoning. Most Recently, I am interested in (1) exploring scalable architectures and recipes for LLM/VLMs and multimodal generation as well as (2) studying the self-evolution paradigm of large foundation models. Feel free to contact me for chat or discussion if you are also interested in these topics.
              </p>
              <p>
                Email: lzyxx17 [at] gmail.com
              </p>
              <p style="text-align:left">
                <!-- <a href="lin-zy17@mails.tsinghua.edu.cn">Email</a> &nbsp/&nbsp-->
                <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp
                <a href="data/JonBarron-bio.txt">Biography</a> &nbsp/&nbsp -->
                <a href="https://twitter.com/zy27962986">Twitter</a> &nbsp
                <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=4ahRAd4AAAAJ">Google Scholar</a> &nbsp
                <a href="https://www.linkedin.com/in/johnson-lin-346b06295/">Linkedin</a>
              </p>
            </td>
            <td style="padding:2.5%;width:70%;max-width:70%">
              <a href="images/zongyulin_2.jpg"><img style="width:80%;max-width:80%" alt="profile photo" src="images/zongyulin_2circle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>News</heading>
                <li style="background-color: yellow;">
                  2025-5 STIV: Scalable Text and Image Conditioned Video Generation is accepted by ICCV 2025!
                 </li>
                <li style="background-color: yellow;">
                  2025-5 Two projects are released: <a href="https://arxiv.org/abs/2505.12705">DreamGen</a> and <a href="https://arxiv.org/abs/2505.15659">FLARE</a> to answer: (1) how synthetic videos + latent action learning can boost the generalization of robot policies to new actions and environments and (2) how to integrate latent world modeling into robot policy learning.
                 </li>
                <li style="background-color: yellow;">
                  2025-5 Two papers: QLASS and SparseCL are accepted by ICML 2025~ Great thanks to all the co-first authors and collaborators!</li>
                  <li style="background-color: yellow;">
                  2025-4 Glad to be the core contributor of GROOT-N1: the first open foundation model for Generalist Humanoid Robots @NVIDIA! See <a href="https://arxiv.org/pdf/2503.14734">preprint.</a>  </li>
                  <li>2025-2 Check our latest paper on efficient Inference-time Scaling + Language Agents using process reward modeling at <a href="https://arxiv.org/abs/2502.02584">preprint</a>. </li>
                  <li>2024-12 Check our latest tech report from Ô£øApple on transparent video generation recipe towards Sora is available now at <a href="https://huggingface.co/papers/2412.07730">preprint</a>. </li>
                    <li>2025-1 Two papers accepted by ICLR 2025~ Great thanks to all the collaborators!</li>

                  <li>2024-11 Our recent paper on benchmarking physical commonsense alignment of video generation: VideoPhy: Evaluating Physical Commonsense In Video Generation, get accepted by Vision-Language@NIPS 2024 as Oral, please check our <a href="https://arxiv.org/pdf/2406.03520">preprint</a>.</li>
                  <li>2024-6 Our new paper on contradiction retrieval: SPARSECL: Sparse Contrastive Learning for
                    Contradiction Retrieval is available now at <a href="https://arxiv.org/pdf/2406.10746">preprint</a>. </li>
                  <!-- <li>2024-6 Our new paper: SparseCL will</li> -->
                </ul>
              </td>
            </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Experience</heading>
              <p>
                <strong>Research Intern, NVIDIA</strong>, 2025</p>
                <p>
                &bull; Core contributor of <a href="https://arxiv.org/pdf/2503.14734">GROOT-N1</a>, the first open foundation model for Generalist Humanoid Robots.</p> 
              <p>
              <p>
                <strong>Research Intern, Ô£øApple</strong>, 2024</p>
                <p>
                &bull; Main contributor of <a href="https://huggingface.co/papers/2412.07730">STIV</a>, a large videogen model, outperforming PIKA, GEN-3, KLING on VBench.</p> 
              <p>
              <p>
                <strong>Research Scientist, Moonshot AI </strong>2023, </p>
                <p>
                &bull; Long-context scaling, videogen, main contributor of <a href="https://kimi.moonshot.cn/">KIMI CHAT</a>.</p> 
              <p>
                <strong>Quant Researcher Intern, Ubiquant</strong>, Top Hedge Fund in China. 2022
              <p>  <strong>Research Intern, Sensetime</strong>, China, 2021
              </p>
            </td>
            
        </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research Topic</heading>
              <p>My research interest lies broadly in natural language processing and general machine learning. Most Recently, I am interested in 
                <p>(1) exploring scalable architectures and recipes for LLMs / VLMs and multi-modal generation;
                <p>(2) improving the self-evolution of LLMs / VLMs;
                <p>(3) improve better alignment with the physical world for vision generative models and vision language models
                <!--Representative papers are <span class="highlight">highlighted</span>.-->
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px">
            <heading>Recent Work</heading>


            <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()"></tr>
              <td style="padding:20px; width:45%; vertical-align:middle">
                <div class="one">
                  <div class="two" id="dualrefl_image">
                    <img src="images/qlass2.png" width="320">
                  </div>
                  <img src="images/qlass1.png" width="320">
                </div>
                <script type="text/javascript">
                  let showFirst = false; 
                  
                  setInterval(() => {
                    showFirst = !showFirst; 
                    document.getElementById('dualrefl_image').style.opacity = showFirst ? "1" : "0";
                  }, 2000);
            
                  document.getElementById('dualrefl_image').style.opacity = "0";
                </script>
              </td>

            <td style="padding:20px;width:75%;vertical-align:middle;background-color: yellow;">
              <!-- <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20338"> -->
                <papertitle>QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search
                </papertitle>
              <!-- </a> -->
              <br>
              <strong>Zongyu Lin*</strong>,Yao Tang*, Xingcheng Yao*, Da Yin*, Ziniu Hu, Yizhou Sun, Kai-Wei Chang
               <br>
               *Equal Contribution<br>
              <em>ICML</em> 2025
              <br>
              <a href="https://arxiv.org/pdf/2502.02584">preprint, </a></a>code and model checkpoints will come soon
              <p></p>
              <p>
                Interested in the combination of Inference time scaling + LLM Agent?ü§ñüí≠ Announcing QLASS (Q-guided Language Agent Stepwise Search), a framework that supercharges language agents at inference time. ‚ö°In this work, we build a process reward model to guide open language agents on complex interactive tasks by estimating the Q-value of each step, without any human annotation for process rewards.
              </p>
            </td>
          </tr>

            <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()"></tr>
              <td style="padding:20px; width:45%; vertical-align:middle">
                <div class="one">
                  <div class="two" id="dualrefl_image">
                    <img src="images/stiv_1.png" width="320">
                  </div>
                  <img src="images/stiv_2.png" width="320">
                </div>
                <script type="text/javascript">
                  let showFirst = false; 
                  
                  setInterval(() => {
                    showFirst = !showFirst; 
                    document.getElementById('dualrefl_image').style.opacity = showFirst ? "1" : "0";
                  }, 2000);
            
                  document.getElementById('dualrefl_image').style.opacity = "0";
                </script>
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle;background-color: yellow;">
                <!-- <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20338"> -->
                  <papertitle>STIV: Scalable Text and Image Conditioned Video Generation
                  </papertitle>
                <!-- </a> -->
                <br>
                <strong>Zongyu Lin*</strong>,Wei Liu*, Chen Chen, Jiasen Lu, Wenze Hu, Tsu-Jui Fu, Jesse Allardice, Zhengfeng Lai, Liangchen Song, Bowen Zhang, Cha Chen, Yiran Fei, Yifan Jiang, Lezhi Li, Yizhou Sun, Kai-Wei Chang, Yinfei Yang
                 <br>
                 *Equal Contribution<br>
                <em>ICCV</em>, 2025
                <br>
                <a href="https://arxiv.org/abs/2412.07730">preprint</a>, <a href="https://huggingface.co/papers/2412.07730">huggingface (#1 Paper of the day)</a>
                <p></p>
                <p>
                  <strong>Ô£øApple</strong>'s latest recipe and studies for scalable video generation modelsüî•üî•üî•. In this work, we aim at providing a transparent and detailed recipe üìñ for model architecture, training strategy and data for scalable text-image conditioned video generation. The pretrained T2V and TI2V models outperform SOTA open-sourced, close-sourced models like Gen-3, PIKA, KLING and CogVideoX-5B on VBench.
                </p>
              </td>
            </tr>


            <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()"></tr>
              <td style="padding:20px; width:45%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='dualrefl_image'>
                    <img src='images/videophy-2.png' width="320"></div>
                  <img src='images/videophy-1.png' width="320">
                </div>
                <script type="text/javascript">
                  function dualrefl_start() {
                    document.getElementById('dualrefl_image').style.opacity = "1";
                  }

                  function dualrefl_stop() {
                    document.getElementById('dualrefl_image').style.opacity = "0";
                  }
                  dualrefl_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle;background-color: yellow;">
                <!-- <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20338"> -->
                  <papertitle>VideoPhy: Evaluating Physical Commonsense In Video Generation</papertitle>
                <!-- </a> -->
                <br>
                Hritik Bansal*,
                <strong>Zongyu Lin*</strong>,
                Jing Zhou,
                Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chenfanfu Jiang, Yizhou Sun, Kai-Wei Chang, Aditya Grover
                 <br>
                 *Equal Contribution<br>
                <em>ICLR 2025</em>, Oral paper accepted by VL@NIPS, 2024
                <br>
                <a href="https://arxiv.org/pdf/2406.03520">preprint</a>, <a href="https://videophy.github.io">website</a>, <a href="https://github.com/Hritikbansal/videophy">code</a>
                <p></p>
                <p>
                  We present VideoPhy, a benchmark designed to assess whether the generated videos follow physical commonsense for real-world activities (e.g. marbles will roll down when placed on a slanted surface). Specifically, we curate a list of 688 captions that involve interactions between various material types in the physical world (e.g., solid-solid, solid-fluid, fluid-fluid). 
                </p>
              </td>
            </tr>

            <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()"></tr>
              <td style="padding:20px; width:45%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='dualrefl_image'>
                    <img src='images/ego_1.png' width="320"></div>
                  <img src='images/ego_2.png' width="320">
                </div>
                <script type="text/javascript">
                  function dualrefl_start() {
                    document.getElementById('dualrefl_image').style.opacity = "1";
                  }

                  function dualrefl_stop() {
                    document.getElementById('dualrefl_image').style.opacity = "0";
                  }
                  dualrefl_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <!-- <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20338"> -->
                  <papertitle>MM-Ego: Towards Building Egocentric Multimodal LLMs </papertitle>
                <!-- </a> -->
                <br>
                Hanrong Ye, Haotian Zhang, Erik Daxberger, Lin Chen, <strong>Zongyu Lin</strong>, Yanghao Li, Bowen Zhang, Haoxuan You, Dan Xu, Zhe Gan, Jiasen Lu, Yinfei Yang
                 <br>
                <em>ICLR</em> 2025
                <br>
                <a href="https://arxiv.org/abs/2410.07177">preprint</a>
                <p></p>
                <p>
                  This research aims to comprehensively explore building a multimodal foundation model for egocentric video understanding, covering training data construction, model and evaluation benchmark. 
                </p>
              </td>
            </tr>

            <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()">
              <td style="padding:20px; width:45%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='dualrefl_image'>
                    <img src='images/sparsecl-2.png' width="260"></div>
                  <img src='images/sparsecl-1.png' width="260">
                </div>
                <script type="text/javascript">
                  function dualrefl_start() {
                    document.getElementById('dualrefl_image').style.opacity = "1";
                  }

                  function dualrefl_stop() {
                    document.getElementById('dualrefl_image').style.opacity = "0";
                  }
                  dualrefl_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <!-- <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20338"> -->
                  <papertitle>SPARSECL: Sparse Contrastive Learning for
                    Contradiction Retrieval</papertitle>
                <!-- </a> -->
                <br>
                Haike Xu*,
                <strong>Zongyu Lin*</strong>,Yizhou Sun, Kai-Wei Chang, Piotr Indyk
                 <br>
                 *Equal Contribution<br>
                <em>ICML</em> 2025
                <br>
                <a href="https://arxiv.org/pdf/2406.10746">preprint</a>, <a href="https://sparsecl.github.io">website</a>, <a href="https://github.com/xuhaike/SparseCL">code</a>
                <p></p>
                <p>
                  We propose a novel sparsity-aware contrastive method to solve contradiction retrieval problem, outperforming traditional bi-encoder and cross encoder baselines, which can benefit fact checking and pretraining data filtering. 
                </p>
              </td>
            </tr>

            <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()"></tr>
              <td style="padding:20px; width:45%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='dualrefl_image'>
                    <img src='images/vdebug_1.png' width="320"></div>
                  <img src='images/vdebug_2.png' width="320">
                </div>
                <script type="text/javascript">
                  function dualrefl_start() {
                    document.getElementById('dualrefl_image').style.opacity = "1";
                  }

                  function dualrefl_stop() {
                    document.getElementById('dualrefl_image').style.opacity = "0";
                  }
                  dualrefl_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <!-- <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20338"> -->
                  <papertitle>VDebugger: Harnessing Execution Feedback for Debugging Visual Programs</papertitle>
                <!-- </a> -->
                <br>
                Xueqing Wu, <strong>Zongyu Lin</strong>, Songyan Zhao, Te-Lin Wu, Pan Lu, Nanyun Peng, Kai-Wei Chang
                 <br>
                <em>EMNLP findings</em>, 2024
                <br>
                <a href="https://arxiv.org/pdf/2406.13444">preprint</a>, <a href="https://github.com/shirley-wu/vdebugger/">code</a>
                <p></p>
                <p>
                  We train a generative judge (critique model) to improve visual programs of CodeLLMs.
                </p>
              </td>
            </tr>

            <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()">
              <td style="padding:20px; width:45%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='dualrefl_image'>
                    <img src='images/UD-2.png' width="320"></div>
                  <img src='images/UD-1.png' width="320">
                </div>
                <script type="text/javascript">
                  function dualrefl_start() {
                    document.getElementById('dualrefl_image').style.opacity = "1";
                  }

                  function dualrefl_stop() {
                    document.getElementById('dualrefl_image').style.opacity = "0";
                  }
                  dualrefl_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <!-- <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20338"> -->
                  <papertitle>A Universal Discriminator for Zero-Shot Generalization</papertitle>
                <!-- </a> -->
                <br>
                Haike Xu,
                <strong>Zongyu Lin</strong>,
                Jing Zhou,
                Yanan Zheng, Zhilin Yang
                 <br>
                <em>ACL Oral</em>, 2023
                <br>
                <a href="https://arxiv.org/abs/2211.08099">preprint</a>, <a href="https://github.com/Rafa-zy/UD">code</a>
                <p></p>
                <p>
                  We are the very pioneering work to study how to train the LLM as a reward model and study its zero-shot generalization on different NLP tasks.
                </p>
              </td>
            </tr>


             <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()">
              <td style="padding:20px; width:45%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='dualrefl_image'>
                    <img src='images/improvet0-2.png' width="320"></div>
                  <img src='images/improvet0-1.png' width="320">
                </div>
                <script type="text/javascript">
                  function dualrefl_start() {
                    document.getElementById('dualrefl_image').style.opacity = "1";
                  }

                  function dualrefl_stop() {
                    document.getElementById('dualrefl_image').style.opacity = "0";
                  }
                  dualrefl_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <!-- <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20338"> -->
                  <papertitle>NOT ALL TASKS ARE BORN EQUAL: UNDERSTANDING ZERO-SHOT GENERALIZATION</papertitle>
                <!-- </a> -->
                <br>
                Jing Zhou,
                <strong>Zongyu Lin</strong>,
                Yanan Zheng, Zhilin Yang
                 <br>
                <em>ICLR Spotlight</em>, 2023
                <br>
                <a href="https://openreview.net/forum?id=KGV-GBh8fb">paper</a>, <a href="https://github.com/Rafa-zy/Improving-T0">code</a>
                <!-- <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20338">paper</a> -->
                <p></p>
                <p>
                  We are the very pioneering work to study the task transfer between different NLP tasks using LLMs. We propose several methods to improve the generalization of LLMs on zero-shot tasks from a data-centric perspective.
                </p>
              </td>
            </tr>

            <!-- <heading>Publications</heading> -->
             <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()">
              <td style="padding:20px; width:45%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='dualrefl_image'>
                    <img src='images/sent-2.png' width="320"></div>
                  <img src='images/sent-1.png' width="320">
                </div>
                <script type="text/javascript">
                  function dualrefl_start() {
                    document.getElementById('dualrefl_image').style.opacity = "1";
                  }

                  function dualrefl_stop() {
                    document.getElementById('dualrefl_image').style.opacity = "0";
                  }
                  dualrefl_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <!-- <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20338"> -->
                  <papertitle>Learning to Detect Noisy Labels Using Model-Based Features</papertitle>
                <!-- </a> -->
                <br>
                Zhihao Wang*,
                <strong>Zongyu Lin*</strong>,
                Peiqi Liu, Guidong Zheng, Junjie Wen, Xianxin Chen, Yujun Chen, Zhilin Yang
                (* First Co-Authors)
                <br>
                <em>Findings of EMNLP</em>, 2023
                <br>
                <a href="https://arxiv.org/abs/2211.08099">preprint</a>, <a href="https://github.com/Rafa-zy/UD">code</a>
                <!-- <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20338">paper</a> -->
                <p></p>
                <p>
                  We are the early work to do self-training on LLMs, and we propose a novel model-based feature to detect noisy labels in the self-training process.
                </p>
              </td>
            </tr>
            <td style="padding:20px">
             <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()">
              <td style="padding:20px; width:45%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='dualrefl_image'>
                    <img src='images/hagen-1.png' width="320"></div>
                  <img src='images/hagen-2.png' width="320">
                </div>
                <script type="text/javascript">
                  function dualrefl_start() {
                    document.getElementById('dualrefl_image').style.opacity = "1";
                  }

                  function dualrefl_stop() {
                    document.getElementById('dualrefl_image').style.opacity = "0";
                  }
                  dualrefl_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20338">
                  <papertitle>Hagen: Homophily-aware graph convolutional recurrent network for crime forecasting</papertitle>
                </a>
                <br>
                Chenyu Wang*,
                <strong>Zongyu Lin*</strong>,
                Guozhen Zhang, 
                Xiaochen Yang, 
                Jiao Sun, Mingxuan Yue, Cyrus Shahabi
                (* First Co-Authors)
                <br>
                <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, 2022
                <br>
                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20338">paper</a>, <a href="https://github.com/Rafa-zy/HAGEN">code</a>
                <p></p>
                <p>
                  We propose an end-to-end graph convolutional recurrent network called HAGEN with several novel designs for crime prediction. Specifically, our framework could jointly capture the crime correlation between regions and the temporal crime dynamics by combining an adaptive region graph learning module with the Diffusion Convolution Gated Recurrent Unit (DCGRU). Based on the homophily assumption of GNN (i.e., graph convolution works better where neighboring nodes share the same label), we propose a homophily-aware constraint to regularize the optimization of the region graph so that neighboring region nodes on the learned graph share similar crime patterns.
                </p>
              </td>
            </tr>
             <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()">
              <td style="padding:20px; width:45%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='dualrefl_image'>
                    <img src='images/vehicle-2.png' width="320"></div>
                  <img src='images/vehicle-1.png' width="320">
                </div>
                <script type="text/javascript">
                  function dualrefl_start() {
                    document.getElementById('dualrefl_image').style.opacity = "1";
                  }

                  function dualrefl_stop() {
                    document.getElementById('dualrefl_image').style.opacity = "0";
                  }
                  dualrefl_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://dl.acm.org/doi/abs/10.1145/3474717.3483987">
                  <papertitle>Vehicle Trajectory Recovery on Road Network Based on Traffic Camera Video Data</papertitle>
                </a>
                <br>
                <strong>Zongyu Lin</strong>,
                Guozhen Zhang, 
                Zhiqun He, 
                Jie Feng, 
                Wei Wu, 
                <a href="http://fi.ee.tsinghua.edu.cn/~liyong/">Yong Li</a>
                <br>
                <em>Proceedings of the 29th International Conference on Advances in Geographic Information Systems</em>, 2021
                <br>
                <a href="https://dl.acm.org/doi/pdf/10.1145/3411807">paper</a>
                <p></p>
                <p>
                  We propose a general system to recover vehicle trajectories at the level of the road intersection, where a novel iterative framework is developed to combine both vehicle clustering and trajectory recovery tasks.
                </p>
              </td>
            </tr>
             <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()">
               <td style="padding:20px; width:45%;vertical-align:middle">
                 <div class="one">
                   <div class="two" id='dualrefl_image'>
                     <img src='images/hw_before.png' width="320"></div>
                   <img src='images/hw_after.png' width="320">
                 </div>
                 <script type="text/javascript">
                   function dualrefl_start() {
                     document.getElementById('dualrefl_image').style.opacity = "1";
                   }

                   function dualrefl_stop() {
                     document.getElementById('dualrefl_image').style.opacity = "0";
                   }
                   dualrefl_stop()
                 </script>
               </td>
               <td style="padding:20px;width:75%;vertical-align:middle">
                 <a href="https://dl.acm.org/doi/abs/10.1145/3432229">
                   <papertitle>HealthWalks: Sensing Fine-grained Individual Health Condition via Mobility Data</papertitle>
                 </a>
                 <br>
                 <strong>Zongyu Lin</strong>,
                 Shiqing Lyu,
                 <a href="http://sniklaus.com/welcome">Hancheng Cao</a>,
                 Yuqiong Wei,
                 <a href="https://www.cse.ust.hk/~panhui/publications.html">Pan Hui</a>,
                 <a href="http://www.cs.umd.edu/~hjs/">Hanan Samet</a>,
                 <a href="http://fi.ee.tsinghua.edu.cn/~liyong/">Yong Li</a>
                 <br>
                 <em>In ACM International Joint Conference on Pervasive and Ubiquitous Computing (UBICOMP)</em>, 2020
                 <br>
                 <!-- <a href="http://sniklaus.com/dualref">project page</a> / -->
                 <a href="https://dl.acm.org/doi/abs/10.1145/3432229">paper</a>
                 <p></p>
                 <p>
                   We propose a DFA-based model which can generate interpretable features automatically from raw mobility data for fine-grained health sensing.
               </td>
             </tr>
          <!-- <tr onmouseout="lssr_stop()" onmouseover="lssr_start()">
            <td style="padding:20px; width:45%;vertical-align:middle">
              <div class="one">
                <div class="two" id='lssr_image'>
                  <img src='images/sume_after.png' width="320"></div>
                <img src='images/sume_before.png' width="320">
              </div>
              <script type="text/javascript">
                function lssr_start() {
                  document.getElementById('lssr_image').style.opacity = "1";
                }

                function lssr_stop() {
                  document.getElementById('lssr_image').style.opacity = "0";
                }
                lssr_stop()
              </script>
            </td> -->
            <!-- <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://dl.acm.org/doi/pdf/10.1145/3411807">
                <papertitle>SUME: Semantic-enhanced Urban Mobility Network Embedding for User Demographic Inference</papertitle>
              </a>
              <br>
              <a href="http://fenglixu.com">Fengli Xu*</a>,
              <strong>Zongyu Lin*</strong>,
              Tong Xia,
              Diansheng Guo,
              <a href="http://fi.ee.tsinghua.edu.cn/~liyong/">Yong Li</a>
              (* Equal Contributions)
              <br>
              <em>In ACM International Joint Conference on Pervasive and Ubiquitous Computing (UBICOMP)</em>, 2020
              <br>
              <a href="https://dl.acm.org/doi/pdf/10.1145/3411807">paper</a>
              <p></p>
              <p>
                We propose a semantic-enhanced urban mobility embedding model for user profiling, and reveal meaningful patterns in all spatial, temporal and urban structure domains.
              </p>
            </td> -->
          </tr>

          <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
            <td style="padding:20px; width:45%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nlt_image'>
                  <!--
                  <video  width=100% height=100% muted autoplay loop>
                <source src="images/cf_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video></div>-->
                <img src='images/cf_before.png' width="320">
                </div>
                <img src='images/cf_after.png' width="320">
              </div>
              <script type="text/javascript">
                function nlt_start() {
                  document.getElementById('nlt_image').style.opacity = "1";
                }

                function nlt_stop() {
                  document.getElementById('nlt_image').style.opacity = "0";
                }
                nlt_stop()
              </script>
            </td>
            <!-- <td style="padding:20px;width:75%;vertical-align:middle">
              <!--<a href="http://nlt.csail.mit.edu/">
                <papertitle>CrimeForecaster: Crime Prediction by Exploiting the Neighborhoods‚Äô Spatiotemporal Dependencies</papertitle>
              </a>-->
              <papertitle>CrimeForecaster: Crime Prediction by Exploiting the Neighborhoods‚Äô Spatiotemporal Dependencies</papertitle>
              <br>
              Jiao Sun,
              Mingxuan Yue,
              <!--<a href="https://sunjiao123sun.github.io/#publication">Jiao Sun</a>,-->
              <strong>Zongyu Lin</strong>,
              Xiaochen Yang,
              Gabe Kahn,
              Luciano Nocera,
              <a href="http://billf.mit.edu/">Cyrus Shahabi</a>
              <br>
              <em>The European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD) </em>, 2020
              <br>
              <!--<a href="http://nlt.csail.mit.edu/">project page</a> /
              <a href="https://arxiv.org/abs/2008.03806">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=OGEnCWZihHE">video</a>-->
              <p></p>
              <p>We introduce a new end-to-end spatiotemporal learning framework dubbed CrimeForecaster that: 1) represents the geographical extents of neighborhoods and their correlations in a graph; 2) uses graph convolution to predict crimes.</p>
            </td>
          </tr> -->

        </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Selected Awards</heading>
                <p>
                  <strong>Comprehensive Outstanding Scholarship(~10/280)</strong>, Tsinghua University. 2020
                <p>  <strong>Excellent Technology Innovation Scholarship</strong>, Tsinghua University. 2020
                <p>  <strong>First Prize in Software Design Contest</strong>, Department of Electronic Enginnering, Tsinghua University. 2018
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Services</heading>
                <p>
                  <strong>Teaching Assistant</strong></p>
                  <p>
                  &bull; Natural Language Processing Seminar(Spring 2022). Teaching assistant. Instructor: Prof. Zhilin Yang. Tsinghua University
                </p> 
                <p>
                <p>
                  <strong>Mentoring </strong> </p>
                  <p>
                  &bull; Xiaohan Song (2024), undergraduate at UCLA</p> 
                  &bull; Yao Tang (2024), undergraduate at SJTU, now a PhD at UPenn</p> 
                  &bull; Guang Yang (2022), undergraduate at Tsinghua University, now a PhD at UW</p> 
                  &bull; Haike Xu (2022), undergraduate at Tsinghua University, now a PhD at MIT</p> 
                  &bull; Chenyu Wang (2021), undergraduate at Tsinghua University, now a PhD at MIT</p> 
                <p>
                  <p>
                    <strong>Reviewer </strong> </p>
                    <p>
                    &bull; 2025: ICML, ICLR, ACL, NeurIPS</p> 
                    &bull; 2024: NeurIPS, ACL, EMNLP</p>
                    &bull; 2023: ACL, EMNLP</p> 
                  <p>
     
                </p>
              </td>
              
          </tr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Hobbies</heading>
                <p>
                  <strong>Sports!</strong> I really enjoy playing ballgames like football and tennis. I am a big fan of Lionel Messi, Rafael Nadal and Stephen Curry! Also, I love running, swimming and hiking.
                <!--<p>  <strong>Ballroom Dancing</strong>, Tsinghua University. 2020-->

                </p>
              </td>
            </tr>
          </tbody></table>
<p> Updated at June.2024. Thanks <a href="https://jonbarron.info">Jon Barron</a> for this concise and beautiful template.
<!--
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
